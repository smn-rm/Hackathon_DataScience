{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:46:32.581451400Z",
     "start_time": "2023-12-08T15:46:32.363508400Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "from adl import DataProcessor\n",
    "from adl import AnomalyDetector, CosineSimilarityCalculator, PropensityScorer\n",
    "from adl import compare_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process and load data for the different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:46:32.585319500Z",
     "start_time": "2023-12-08T15:46:32.582813700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to Wuerth Data\n",
    "DATA_PATH = \"../01_data/dataset_wuerth.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:46:34.517797500Z",
     "start_time": "2023-12-08T15:46:32.585319500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No scaling applied\n",
      "No scaling applied\n",
      "No scaling applied\n",
      "No scaling applied\n",
      "Normalizing applied\n",
      "Normalizing applied\n",
      "Normalizing applied\n",
      "Normalizing applied\n",
      "Unbiased standardizing applied\n",
      "Unbiased standardizing applied\n",
      "Unbiased standardizing applied\n",
      "Unbiased standardizing applied\n"
     ]
    }
   ],
   "source": [
    "processor = DataProcessor(DATA_PATH)\n",
    "\n",
    "combined_condition = lambda df: (df[\"dunning_level_max\"] > 2) & (\n",
    "    df[\"dunning_level_current\"] > 2\n",
    ")\n",
    "# For Anomaly Detection\n",
    "data_full = processor.process_data(mode=None, scale=None)\n",
    "data_remove = processor.process_data(\n",
    "    mode=\"remove\", scale=None, conditions=combined_condition\n",
    ")\n",
    "data_extract = processor.process_data(\n",
    "    mode=\"extract\", scale=None, target_variable=\"flag_new_orsyshelf\"\n",
    ")\n",
    "data_both = processor.process_data(\n",
    "    mode=\"both\", scale=None, conditions=combined_condition\n",
    ")\n",
    "\n",
    "# For Cosine Similarity\n",
    "data_full_normalized = processor.process_data(mode=None, scale=\"normalize\")\n",
    "data_remove_normalized = processor.process_data(\n",
    "    mode=\"remove\", scale=\"normalize\", conditions=combined_condition\n",
    ")\n",
    "data_extract_normalized = processor.process_data(\n",
    "    mode=\"extract\", scale=\"normalize\", target_variable=\"flag_new_orsyshelf\"\n",
    ")\n",
    "data_both_normalized = processor.process_data(\n",
    "    mode=\"both\", scale=\"normalize\", conditions=combined_condition\n",
    ")\n",
    "\n",
    "# For Propensity scoring\n",
    "data_full_standardized = processor.process_data(mode=None, scale=\"standardize\")\n",
    "data_remove_standardized = processor.process_data(\n",
    "    mode=\"remove\", scale=\"standardize\", conditions=combined_condition\n",
    ")\n",
    "data_extract_standardized = processor.process_data(\n",
    "    mode=\"extract\", scale=\"standardize\", target_variable=\"flag_new_orsyshelf\"\n",
    ")\n",
    "data_both_standardized = processor.process_data(\n",
    "    mode=\"both\", scale=\"standardize\", conditions=combined_condition\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting potential customers\n",
    "## Anomaly Detection\n",
    "\n",
    "1. Calculating anomalies over 500 variations and counting how often a customer has been selected \n",
    "2. Storing the ordered data with the count how often a customer has been selected and is above a certain amount in 01_data/data_out (in order to save computational time when evaluating our method)\n",
    "\n",
    "*Note: Uncomment line 27 in the following cell to compute the anomaly counts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:46:34.523804900Z",
     "start_time": "2023-12-08T15:46:34.520300100Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_CPU = 4\n",
    "\n",
    "anomaly_detector = AnomalyDetector()\n",
    "\n",
    "count_threshold = [250, 200, 150, 100]\n",
    "\n",
    "\n",
    "# Calculate cust_id_counts for each dataframe and directly save them to csv to\n",
    "# save computational time when calling the script again\n",
    "\n",
    "\n",
    "def process_and_save(data, i, suffix):\n",
    "    anomalies = anomaly_detector.extract_anomalies(data, count_threshold=i)\n",
    "    anomalies.to_csv(f\"../01_data/data_out/anomaly_{suffix}_{i}.csv\", index=False)\n",
    "\n",
    "\n",
    "def run_anomaly():\n",
    "    with ThreadPoolExecutor(max_workers=MAX_CPU) as executor:\n",
    "        for i in count_threshold:\n",
    "            executor.submit(process_and_save, data_full, i, \"full\")\n",
    "            executor.submit(process_and_save, data_remove, i, \"remove\")\n",
    "            executor.submit(process_and_save, data_extract, i, \"extract\")\n",
    "            executor.submit(process_and_save, data_both, i, \"both\")\n",
    "\n",
    "\n",
    "# running will take some time -> results stored in data_out\n",
    "# run_anomaly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call anomaly detection results from csv to save computational time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:46:34.569452900Z",
     "start_time": "2023-12-08T15:46:34.524801700Z"
    }
   },
   "outputs": [],
   "source": [
    "count = 150\n",
    "# -> We decided to classify sth as anomaly if it has been detected in at least\n",
    "# 30% of the different iterations\n",
    "\n",
    "anomaly_full = pd.read_csv(\n",
    "    f\"../01_data/data_out/anomaly_full_{count}.csv\", index_col=False\n",
    ")\n",
    "anomaly_remove = pd.read_csv(\n",
    "    f\"../01_data/data_out/anomaly_remove_{count}.csv\", index_col=False\n",
    ")\n",
    "anomaly_extract = pd.read_csv(\n",
    "    f\"../01_data/data_out/anomaly_extract_{count}.csv\", index_col=False\n",
    ")\n",
    "anomaly_both = pd.read_csv(\n",
    "    f\"../01_data/data_out/anomaly_both_{count}.csv\", index_col=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "1. Compute the pairwise cosine similarity of each non orsy customer with every orsy customer and store those\n",
    "2. Store either the average or the count how often the cosine similarity exceeded a certain threshold (the latter was used here) and this returns a dataframe witht the ordered cust_ids\n",
    "3. The top n non orsy customers are returned where n equals the number of customers who exceeded the threshold in anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:46:37.728865100Z",
     "start_time": "2023-12-08T15:46:34.570693500Z"
    }
   },
   "outputs": [],
   "source": [
    "csc = CosineSimilarityCalculator()\n",
    "\n",
    "cosine_full = csc.get_both_cosine_metrics(\n",
    "    data_full_normalized, threshold=0.8, n_best=anomaly_full.shape[0], sort_by=\"count\"\n",
    ")\n",
    "cosine_remove = csc.get_both_cosine_metrics(\n",
    "    data_remove_normalized,\n",
    "    threshold=0.8,\n",
    "    n_best=anomaly_remove.shape[0],\n",
    "    sort_by=\"count\",\n",
    ")\n",
    "cosine_extract = csc.get_both_cosine_metrics(\n",
    "    data_extract_normalized,\n",
    "    threshold=0.8,\n",
    "    n_best=anomaly_extract.shape[0],\n",
    "    sort_by=\"count\",\n",
    ")\n",
    "cosine_both = csc.get_both_cosine_metrics(\n",
    "    data_both_normalized, threshold=0.8, n_best=anomaly_both.shape[0], sort_by=\"count\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propensity Scoring\n",
    "\n",
    "1. Get the tuned hyperparameters from `03_model_selection.ipynb`\n",
    "2. Perform gradient boosting and extract the top n non orsy customers according to their likeliness of being an orsy customer.\n",
    "3. The top n non orsy customers are returned where n equals the number of customers who exceeded the threshold in anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:46:37.746436Z",
     "start_time": "2023-12-08T15:46:37.729878200Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"../01_data/data_model_eval/full_best_params.pkl\", \"rb\") as file:\n",
    "    full_best_params = pickle.load(file)\n",
    "\n",
    "with open(\"../01_data/data_model_eval/remove_best_params.pkl\", \"rb\") as file:\n",
    "    remove_best_params = pickle.load(file)\n",
    "\n",
    "with open(\"../01_data/data_model_eval/extract_best_params.pkl\", \"rb\") as file:\n",
    "    extract_best_params = pickle.load(file)\n",
    "\n",
    "with open(\"../01_data/data_model_eval/both_best_params.pkl\", \"rb\") as file:\n",
    "    both_best_params = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:51:01.442498600Z",
     "start_time": "2023-12-08T15:46:37.749495800Z"
    }
   },
   "outputs": [],
   "source": [
    "propensity_scorer = PropensityScorer()\n",
    "\n",
    "gb_full = propensity_scorer.gradient_boosting(\n",
    "    data_full_standardized, n_best=anomaly_full.shape[0], **full_best_params\n",
    ")\n",
    "gb_remove = propensity_scorer.gradient_boosting(\n",
    "    data_remove_standardized, n_best=anomaly_remove.shape[0], **remove_best_params\n",
    ")\n",
    "gb_extract = propensity_scorer.gradient_boosting(\n",
    "    data_extract_standardized, n_best=anomaly_extract.shape[0], **extract_best_params\n",
    ")\n",
    "gb_both = propensity_scorer.gradient_boosting(\n",
    "    data_both_standardized, n_best=anomaly_both.shape[0], **both_best_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Models\n",
    "\n",
    "1. For each data set we compare which non orsy customer was predicted as an orsy customer\n",
    "2. 1 behind a cust_id means only predicted by one model, 2 by two and so on. There are alsdo columns indicating which model predicted someone to be a potential orsy customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:51:01.468339900Z",
     "start_time": "2023-12-08T15:51:01.446011700Z"
    }
   },
   "outputs": [],
   "source": [
    "df_names = [\"cosine\", \"anomaly\", \"propensity\"]\n",
    "\n",
    "common_full = compare_results(\n",
    "    dataframes=[cosine_full, anomaly_full, gb_full], df_names=df_names\n",
    ")\n",
    "common_remove = compare_results([cosine_remove, anomaly_remove, gb_remove], df_names)\n",
    "common_extract = compare_results(\n",
    "    [cosine_extract, anomaly_extract, gb_extract], df_names\n",
    ")\n",
    "common_both = compare_results([cosine_both, anomaly_both, gb_both], df_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T15:51:01.486396800Z",
     "start_time": "2023-12-08T15:51:01.469348500Z"
    }
   },
   "outputs": [],
   "source": [
    "common_full.to_csv(\"../01_data/data_out/common_full.csv\", index=False)\n",
    "common_remove.to_csv(\"../01_data/data_out/common_remove.csv\", index=False)\n",
    "common_extract.to_csv(\"../01_data/data_out/common_extract.csv\", index=False)\n",
    "common_both.to_csv(\"../01_data/data_out/common_both.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
